# -*- coding: utf-8 -*-
"""ff.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkenVQ9wGqtjti4lz2UXabHYvEQbuQQZ
"""

import os
import pandas as pd
from PIL import Image
import math
import numpy as np
import cv2
import keras
import seaborn as sns
from keras.layers import Dense, Dropout, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import BatchNormalization
from keras.optimizers import Adam
from keras.models import Sequential
import tensorflow as tf
import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
np.random.seed(42)

from matplotlib import style

style.use('fivethirtyeight')

print("current directory: ", os.getcwd())

TextFilesLocation = [f for f in os.listdir('/content/annotations') if f.endswith('.xml')]
ImageFileLocation = [f for f in os.listdir('/content/images') if f.endswith('.png')]
# sort the lists
TextFilesLocation.sort()
ImageFileLocation.sort()


print(len(TextFilesLocation),len(ImageFileLocation))

ImageContent = []
for f in ImageFileLocation:
    img = Image.open("/content/images/"+f)

    img = np.array(img)
    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)


    resized_image = cv2.resize(gray_image, (28, 28))

    # Flatten the image into a 1D array
    vector = resized_image.flatten()

    # Append the vector to the list of vectors
    #vectors.append(vector)

    ImageContent.append(vector)

ImageContent = np.array(ImageContent)
print(ImageContent.shape)

x=ImageContent.reshape(-1,28,28,1)

# read the text files and store the content as a list called TextContent
TextContent = []
for f in TextFilesLocation:
    with open("/content/annotations/"+f, 'r') as file:
        TextContent.append(file.read())

import os
import pandas as pd
import xml.etree.ElementTree as ET

# define the folder containing the XML files
folder_path = '/content/annotations/'

# create an empty list to hold the extracted data
data = []

# loop over all the files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.xml'):
        # parse the XML file using ElementTree
        tree = ET.parse(os.path.join(folder_path, filename))
        root = tree.getroot()

        # extract the relevant data from the first object in the XML file
        obj = root.find('object')
        if obj is not None:
            name = obj.find('name').text
            xmin = int(obj.find('bndbox/xmin').text)
            ymin = int(obj.find('bndbox/ymin').text)
            xmax = int(obj.find('bndbox/xmax').text)
            ymax = int(obj.find('bndbox/ymax').text)

            # add the data to the list
            data.append({'filename': filename,
                         'name': name,
                         'xmin': xmin,
                         'ymin': ymin,
                         'xmax': xmax,
                         'ymax': ymax})

# convert the list to a Pandas DataFrame
df = pd.DataFrame(data)

classes = {"name":{ "trafficlight":0,
            "speedlimit":1,
            "crosswalk":2,
            'stop':3}}

IMG_HEIGHT = 28
IMG_WIDTH = 28

df = df.replace(classes)
labels=df['name']
arr=labels.to_numpy()

X_train, X_val, y_train, y_val = train_test_split(x, arr, test_size=0.3, random_state=42, shuffle=True)

X_train = X_train/255
X_val = X_val/255

print("X_train.shape", X_train.shape)
print("X_valid.shape", X_val.shape)
print("y_train.shape", y_train.shape)
print("y_valid.shape", y_val.shape)

y_train = keras.utils.to_categorical(y_train, 4)
y_val = keras.utils.to_categorical(y_val, 4)

print(y_train.shape)
print(y_val.shape)

import tensorflow as tf
from tensorflow.keras import layers, models

model2 = models.Sequential()
model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model2.add(layers.BatchNormalization())
model2.add(layers.Conv2D(64, (3, 3), activation='relu'))
model2.add(layers.BatchNormalization())
model2.add(layers.MaxPooling2D((2, 2)))
model2.add(layers.Dropout(0.25))
model2.add(layers.Conv2D(128, (3, 3), activation='relu'))
model2.add(layers.BatchNormalization())
model2.add(layers.MaxPooling2D((2, 2)))
model2.add(layers.Dropout(0.25))
model2.add(layers.Flatten())
model2.add(layers.Dense(256, activation='relu'))
model2.add(layers.BatchNormalization())
model2.add(layers.Dropout(0.5))
model2.add(layers.Dense(4, activation='softmax'))

# Compile the model
model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
history2 = model2.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))

# Print training and testing accuracy
train_acc = history2.history['accuracy']
val_acc = history2.history['val_accuracy']

print("Training Accuracy: ", train_acc[-1])
print("Testing Accuracy: ", val_acc[-1])

# Extract the output of the last CNN layer
cnn_output = model2.layers[-2].output

# Define the MLP classifier
mlp = models.Sequential()
mlp.add(layers.Dense(128, activation='relu'))
mlp.add(layers.Dense(64, activation='relu'))
mlp.add(layers.Dense(4, activation='softmax'))  # Replace 'num_classes' with the actual number of classes

# Pass the CNN output to the MLP classifier
mlp_output = mlp(cnn_output)

# Define the combined model
combined_model = models.Model(inputs=model2.input, outputs=mlp_output)

# Compile the combined model
combined_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                       loss='categorical_crossentropy',
                       metrics=['accuracy'])

# Train the combined model
history_combined = combined_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))

# Print training and testing accuracy
train_acc = history_combined.history['accuracy'][-1]
val_acc = history_combined.history['val_accuracy'][-1]

print("Training Accuracy:", train_acc)
print("Testing Accuracy:", val_acc)

combined_model.save('combined_model.h5')

predictions = combined_model.predict(X_val)
predictions = np.argmax(predictions, axis=1)

# Print the predicted array
print("Predicted Label:")
print(predictions)

import streamlit as st
import tensorflow as tf
from PIL import Image
import numpy as np

# Load the combined model
combined_model = tf.keras.models.load_model('combined_model.h5')

# Streamlit app
def main():
    st.title("Image Classification with CNN-MLP")
    st.write("Upload an image and get the predicted class.")

    # File uploader
    uploaded_file = st.file_uploader("Choose an image file", type=["png", "jpg", "jpeg"])

    if uploaded_file is not None:
        # Read the uploaded image
        image = Image.open(uploaded_file)
        st.image(image, caption='Uploaded Image', use_column_width=True)

        # Preprocess the image
        image = image.resize((32, 32))  # Resize the image to match the input size of the model
        image = np.array(image) / 255.0  # Normalize the image
        image = np.expand_dims(image, axis=0)  # Add an extra dimension for the batch

        # Use the combined model to predict the class
        prediction = combined_model.predict(image)
        class_label = np.argmax(prediction)  # Get the class label with the highest probability

        # Display the predicted class
        st.write("Predicted Class:")
        st.write(class_label)

if __name__ == '__main__':
    main()